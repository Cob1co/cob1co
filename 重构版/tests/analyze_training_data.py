"""åˆ†æè®­ç»ƒæ•°æ®è´¨é‡

æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒã€å¼‚å¸¸å€¼ã€æ ·æœ¬è´¨é‡
"""

import pickle
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


def analyze_training_data(data_path):
    """åˆ†æè®­ç»ƒæ•°æ®"""
    
    print("="*70)
    print("ğŸ“Š è®­ç»ƒæ•°æ®è´¨é‡åˆ†æ")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“‹ åŠ è½½æ•°æ®...")
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    features = np.array(data['features'])
    labels = np.array(data['labels'])
    
    print(f"âœ… æ ·æœ¬æ•°: {len(labels)}")
    print(f"âœ… ç‰¹å¾å½¢çŠ¶: {features.shape}")
    print(f"âœ… æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
    
    # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
    print("\nğŸ“Š æ ‡ç­¾ç»Ÿè®¡åˆ†æ")
    print("-"*70)
    
    alpha_soc = labels[:, 0]
    alpha_grid = labels[:, 1]
    alpha_cost = labels[:, 2]
    
    print(f"\nÎ±_soc (SOCè·Ÿè¸ªæƒé‡):")
    print(f"  å‡å€¼: {np.mean(alpha_soc):.3f}")
    print(f"  æ ‡å‡†å·®: {np.std(alpha_soc):.3f}")
    print(f"  æœ€å°å€¼: {np.min(alpha_soc):.3f}")
    print(f"  æœ€å¤§å€¼: {np.max(alpha_soc):.3f}")
    print(f"  ä¸­ä½æ•°: {np.median(alpha_soc):.3f}")
    
    print(f"\nÎ±_grid (ç”µç½‘è·Ÿè¸ªæƒé‡):")
    print(f"  å‡å€¼: {np.mean(alpha_grid):.3f}")
    print(f"  æ ‡å‡†å·®: {np.std(alpha_grid):.3f}")
    print(f"  æœ€å°å€¼: {np.min(alpha_grid):.3f}")
    print(f"  æœ€å¤§å€¼: {np.max(alpha_grid):.3f}")
    print(f"  ä¸­ä½æ•°: {np.median(alpha_grid):.3f}")
    
    print(f"\nÎ±_cost (æˆæœ¬æƒé‡):")
    print(f"  å‡å€¼: {np.mean(alpha_cost):.3f}")
    print(f"  æ ‡å‡†å·®: {np.std(alpha_cost):.3f}")
    print(f"  æœ€å°å€¼: {np.min(alpha_cost):.3f}")
    print(f"  æœ€å¤§å€¼: {np.max(alpha_cost):.3f}")
    print(f"  ä¸­ä½æ•°: {np.median(alpha_cost):.3f}")
    
    # åˆ†ææ ‡ç­¾ç»„åˆ
    print("\nğŸ“Š æ ‡ç­¾ç»„åˆåˆ†æ")
    print("-"*70)
    
    # ç»Ÿè®¡æ¯ç§ç»„åˆå‡ºç°çš„æ¬¡æ•°
    from collections import Counter
    label_tuples = [tuple(label) for label in labels]
    label_counts = Counter(label_tuples)
    
    print(f"ä¸åŒæ ‡ç­¾ç»„åˆæ•°: {len(label_counts)}")
    print(f"\nå‰10ä¸ªæœ€å¸¸è§çš„æ ‡ç­¾ç»„åˆ:")
    for i, (label, count) in enumerate(label_counts.most_common(10), 1):
        pct = count / len(labels) * 100
        print(f"  {i}. {label} - {count}æ¬¡ ({pct:.1f}%)")
    
    # åˆ†æåç¦»ç¨‹åº¦
    print("\nğŸ“Š æƒé‡åç¦»åŸºå‡†(1.0)çš„åˆ†æ")
    print("-"*70)
    
    deviations = np.mean(np.abs(labels - 1.0), axis=1)
    
    normal = np.sum(deviations < 0.1)
    important = np.sum((deviations >= 0.1) & (deviations < 0.3))
    extreme = np.sum(deviations >= 0.3)
    
    print(f"æ™®é€šæ ·æœ¬ (<0.1): {normal} ({normal/len(labels)*100:.1f}%)")
    print(f"é‡è¦æ ·æœ¬ (0.1-0.3): {important} ({important/len(labels)*100:.1f}%)")
    print(f"æç«¯æ ·æœ¬ (>0.3): {extreme} ({extreme/len(labels)*100:.1f}%)")
    
    print(f"\nå¹³å‡åç¦»åº¦: {np.mean(deviations):.3f}")
    print(f"æœ€å¤§åç¦»åº¦: {np.max(deviations):.3f}")
    print(f"æœ€å°åç¦»åº¦: {np.min(deviations):.3f}")
    
    # å¯è§†åŒ–
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # 1. Î±_socç›´æ–¹å›¾
    axes[0, 0].hist(alpha_soc, bins=50, edgecolor='black', alpha=0.7)
    axes[0, 0].axvline(1.0, color='r', linestyle='--', label='åŸºå‡†(1.0)')
    axes[0, 0].set_xlabel('Î±_soc')
    axes[0, 0].set_ylabel('é¢‘æ•°')
    axes[0, 0].set_title('Î±_socåˆ†å¸ƒ')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Î±_gridç›´æ–¹å›¾
    axes[0, 1].hist(alpha_grid, bins=50, edgecolor='black', alpha=0.7)
    axes[0, 1].axvline(1.0, color='r', linestyle='--', label='åŸºå‡†(1.0)')
    axes[0, 1].set_xlabel('Î±_grid')
    axes[0, 1].set_ylabel('é¢‘æ•°')
    axes[0, 1].set_title('Î±_gridåˆ†å¸ƒ')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Î±_costç›´æ–¹å›¾
    axes[0, 2].hist(alpha_cost, bins=50, edgecolor='black', alpha=0.7)
    axes[0, 2].axvline(1.0, color='r', linestyle='--', label='åŸºå‡†(1.0)')
    axes[0, 2].set_xlabel('Î±_cost')
    axes[0, 2].set_ylabel('é¢‘æ•°')
    axes[0, 2].set_title('Î±_coståˆ†å¸ƒ')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. Î±_soc vs Î±_gridæ•£ç‚¹å›¾
    axes[1, 0].scatter(alpha_soc, alpha_grid, alpha=0.3, s=10)
    axes[1, 0].axvline(1.0, color='r', linestyle='--', alpha=0.5)
    axes[1, 0].axhline(1.0, color='r', linestyle='--', alpha=0.5)
    axes[1, 0].set_xlabel('Î±_soc')
    axes[1, 0].set_ylabel('Î±_grid')
    axes[1, 0].set_title('Î±_soc vs Î±_grid')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. Î±_soc vs Î±_costæ•£ç‚¹å›¾
    axes[1, 1].scatter(alpha_soc, alpha_cost, alpha=0.3, s=10)
    axes[1, 1].axvline(1.0, color='r', linestyle='--', alpha=0.5)
    axes[1, 1].axhline(1.0, color='r', linestyle='--', alpha=0.5)
    axes[1, 1].set_xlabel('Î±_soc')
    axes[1, 1].set_ylabel('Î±_cost')
    axes[1, 1].set_title('Î±_soc vs Î±_cost')
    axes[1, 1].grid(True, alpha=0.3)
    
    # 6. åç¦»åº¦ç›´æ–¹å›¾
    axes[1, 2].hist(deviations, bins=50, edgecolor='black', alpha=0.7)
    axes[1, 2].axvline(0.1, color='orange', linestyle='--', label='æ™®é€š/é‡è¦é˜ˆå€¼')
    axes[1, 2].axvline(0.3, color='red', linestyle='--', label='é‡è¦/æç«¯é˜ˆå€¼')
    axes[1, 2].set_xlabel('å¹³å‡åç¦»åº¦')
    axes[1, 2].set_ylabel('é¢‘æ•°')
    axes[1, 2].set_title('æƒé‡åç¦»åº¦åˆ†å¸ƒ')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    output_path = Path(__file__).parent.parent / 'LMPC' / 'data' / 'training_data_analysis.png'
    plt.savefig(output_path, dpi=150)
    print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
    
    # æ£€æŸ¥æ½œåœ¨é—®é¢˜
    print("\nâš ï¸  æ½œåœ¨é—®é¢˜æ£€æŸ¥")
    print("-"*70)
    
    issues = []
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
    if np.max(alpha_soc) > 1.5 or np.min(alpha_soc) < 0.5:
        issues.append("Î±_socå­˜åœ¨è¶…å‡ºåˆç†èŒƒå›´çš„å¼‚å¸¸å€¼")
    
    if np.max(alpha_grid) > 1.5 or np.min(alpha_grid) < 0.5:
        issues.append("Î±_gridå­˜åœ¨è¶…å‡ºåˆç†èŒƒå›´çš„å¼‚å¸¸å€¼")
    
    if np.max(alpha_cost) > 1.5 or np.min(alpha_cost) < 0.5:
        issues.append("Î±_costå­˜åœ¨è¶…å‡ºåˆç†èŒƒå›´çš„å¼‚å¸¸å€¼")
    
    # 2. æ£€æŸ¥åˆ†å¸ƒæ˜¯å¦è¿‡äºé›†ä¸­
    if len(label_counts) < 10:
        issues.append(f"æ ‡ç­¾ç»„åˆè¿‡å°‘({len(label_counts)}ç§)ï¼Œå¯èƒ½æ•°æ®å¤šæ ·æ€§ä¸è¶³")
    
    # 3. æ£€æŸ¥æ˜¯å¦æœ‰å€™é€‰å€¼å æ¯”è¿‡é«˜
    unique_soc, counts_soc = np.unique(alpha_soc, return_counts=True)
    max_count_soc = np.max(counts_soc)
    if max_count_soc / len(labels) > 0.5:
        issues.append(f"Î±_socæœ‰å•ä¸ªå€¼å æ¯”è¿‡é«˜({max_count_soc/len(labels)*100:.1f}%)")
    
    # 4. æ£€æŸ¥æ™®é€šæ ·æœ¬æ¯”ä¾‹
    if normal == 0:
        issues.append("âš ï¸ æ²¡æœ‰æ™®é€šæ ·æœ¬(åç¦»<0.1)ï¼Œæ‰€æœ‰æ ·æœ¬éƒ½éœ€è¦å¤§å¹…è°ƒæ•´æƒé‡")
    
    if len(issues) == 0:
        print("âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
    else:
        print(f"âŒ å‘ç° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜:")
        for i, issue in enumerate(issues, 1):
            print(f"  {i}. {issue}")
    
    print("\n" + "="*70)
    print("åˆ†æå®Œæˆï¼")
    print("="*70)


if __name__ == '__main__':
    data_path = Path(__file__).parent.parent / 'LMPC' / 'data' / 'training_data_30days.pkl'
    analyze_training_data(data_path)
