"""å…¨å¹´è¯„ä¼°è„šæœ¬ eval_annual.py

ç”¨äºåœ¨èšç±»åçš„å…¨å¹´æ•°æ®ä¸Šï¼Œå¯¹ç¬¬äºŒé˜¶æ®µè®­ç»ƒå¾—åˆ°çš„æ‰€æœ‰ä¸“å®¶ç­–ç•¥åšæ±‡æ€»è¯„ä¼°ã€‚

è®¾è®¡ç›®æ ‡ï¼š
- å¯¹æ¯ä¸ªä¸“å®¶å•ç‹¬è°ƒç”¨ eval_expert.evaluate_expert è·å–æŒ‡æ ‡å‡å€¼ï¼›
- åˆ©ç”¨ clustered_training_data.csv ä¸­æ¯ä¸ª Day_Label å¯¹åº”çš„å¤©æ•°ï¼Œ
  æŒ‰å¤©æ•°ä½œä¸ºæƒé‡ï¼Œå¯¹å„ä¸“å®¶æŒ‡æ ‡åšå¹´åº¦åŠ æƒå¹³å‡ï¼›
- å°†ç»“æœä¿å­˜åˆ° eval_results/eval_annual.jsonï¼Œå¹¶åœ¨ç»ˆç«¯æ‰“å°ç®€è¦æ±‡æ€»ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python eval_annual.py
    python eval_annual.py --episodes 50
    python eval_annual.py --config phase2_config.yaml --stochastic
"""

import argparse
import json
from pathlib import Path
from typing import Dict, Any

import pandas as pd
import yaml

from eval_expert import evaluate_expert


def _resolve_path(path_str: str) -> Path:
    """æ ¹æ®é…ç½®ä¸­çš„è·¯å¾„å­—ç¬¦ä¸²è¿”å›å¯ç”¨è·¯å¾„ã€‚

    ä¸ eval_expert.py ä¸­çš„é€»è¾‘ä¿æŒä¸€è‡´ï¼Œå°½é‡å…¼å®¹ç›¸å¯¹è·¯å¾„å†™æ³•ã€‚
    """
    p = Path(path_str)
    if p.exists():
        return p
    # å°è¯•ç›¸å¯¹äºå½“å‰è„šæœ¬ç›®å½•
    alt = Path(__file__).parent / path_str
    if alt.exists():
        return alt
    # å°è¯•åµŒå¥— SAC ç›®å½•ï¼ˆå…¼å®¹æ—§è·¯å¾„ï¼‰
    alt2 = Path(__file__).parent / "SAC" / path_str
    if alt2.exists():
        return alt2
    return p


def _load_clustered_data(config: Dict[str, Any]) -> pd.DataFrame:
    """åŠ è½½èšç±»åçš„å…¨å¹´æ•°æ® clustered_training_data.csvã€‚"""
    data_cfg = config.get("data", {})
    clustered_path_str = data_cfg.get("clustered", "clustered_training_data.csv")
    data_path = _resolve_path(clustered_path_str)
    if not data_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°èšç±»æ•°æ®æ–‡ä»¶: {data_path}")

    df = pd.read_csv(data_path)
    required_cols = {"Date", "Day_Index", "Day_Label"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"clustered_training_data.csv ç¼ºå°‘å¿…è¦åˆ—: {missing}")
    return df


def _compute_days_per_label(df: pd.DataFrame) -> Dict[int, int]:
    """ç»Ÿè®¡æ¯ä¸ª Day_Label å¯¹åº”çš„å¤©æ•°ï¼ˆæŒ‰ Day_Index å»é‡ï¼‰ã€‚"""
    day_info = df[["Date", "Day_Index", "Day_Label"]].drop_duplicates(subset=["Day_Index"])
    group = day_info.groupby("Day_Label")["Day_Index"].nunique()
    return {int(k): int(v) for k, v in group.to_dict().items()}


def main():
    parser = argparse.ArgumentParser(description="å…¨å¹´è¯„ä¼°æ‰€æœ‰ä¸“å®¶ç­–ç•¥")
    parser.add_argument("--config", type=str, default="phase2_config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--episodes", type=int, default=20, help="æ¯ä¸ªä¸“å®¶è¯„ä¼°çš„ episode æ•°é‡")
    parser.add_argument(
        "--stochastic",
        action="store_true",
        help="è¯„ä¼°æ—¶ä½¿ç”¨éšæœºç­–ç•¥ï¼ˆé»˜è®¤ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼‰",
    )
    args = parser.parse_args()

    config_path = Path(args.config)
    if not config_path.exists():
        # å…¼å®¹ç›¸å¯¹è·¯å¾„
        alt = Path(__file__).parent / args.config
        if alt.exists():
            config_path = alt
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {args.config}")

    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    num_experts = int(config.get("training", {}).get("num_experts", 5))

    print("=" * 70)
    print("ğŸš€ å…¨å¹´è¯„ä¼°æ‰€æœ‰ä¸“å®¶ç­–ç•¥")
    print("=" * 70)
    print(f"é…ç½®æ–‡ä»¶: {config_path}")
    print(f"ä¸“å®¶æ•°é‡: {num_experts}")
    print(f"æ¯ä¸ªä¸“å®¶è¯„ä¼° episodes: {args.episodes}")
    print(f"è¯„ä¼°ç­–ç•¥: {'éšæœº' if args.stochastic else 'ç¡®å®šæ€§'}")

    # åŠ è½½èšç±»åçš„å…¨å¹´æ•°æ®ï¼Œç»Ÿè®¡æ¯ä¸ªä¸“å®¶å¯¹åº”çš„å¤©æ•°
    df_clustered = _load_clustered_data(config)
    days_per_label = _compute_days_per_label(df_clustered)
    total_days = int(df_clustered["Day_Index"].nunique())

    print("\nğŸ“Š æ¯ä¸ªä¸“å®¶å¯¹åº”çš„å¤©æ•° (æŒ‰ Day_Label ç»Ÿè®¡):")
    for expert_id in range(num_experts):
        days = days_per_label.get(expert_id, 0)
        print(f"  ä¸“å®¶ {expert_id}: {days} å¤©")

    # å¯¹æ¯ä¸ªä¸“å®¶å•ç‹¬è¯„ä¼°
    expert_summaries: Dict[int, Dict[str, Any]] = {}

    for expert_id in range(num_experts):
        print(f"\n{'=' * 70}")
        print(f"è¯„ä¼°ä¸“å®¶ {expert_id}/{num_experts - 1}")
        print(f"{'=' * 70}")
        summary = evaluate_expert(
            expert_id,
            config,
            episodes=args.episodes,
            deterministic=not args.stochastic,
        )
        expert_summaries[expert_id] = summary

    # ç»Ÿä¸€çš„æŒ‡æ ‡é”®
    metric_keys = ["return", "cost", "curtail", "import", "export", "ramp"]

    # å¹´åº¦åŠ æƒå¹³å‡ï¼ˆæŒ‰å¤©æ•°åŠ æƒï¼‰
    annual_weighted: Dict[str, float] = {}
    for key in metric_keys:
        num = 0.0
        for expert_id, summary in expert_summaries.items():
            days = days_per_label.get(expert_id, 0)
            value = float(summary.get(key, 0.0))
            num += value * days
        if total_days > 0:
            annual_weighted[key] = num / total_days
        else:
            annual_weighted[key] = 0.0

    # æ±‡æ€»ç»“æœ
    result: Dict[str, Any] = {
        "config_path": str(config_path),
        "episodes_per_expert": int(args.episodes),
        "stochastic_policy": bool(args.stochastic),
        "num_experts": num_experts,
        "total_days": total_days,
        "days_per_label": days_per_label,
        "experts": expert_summaries,
        "annual_weighted": annual_weighted,
    }

    # ä¿å­˜åˆ° eval_results/eval_annual.json
    result_dir = Path("eval_results")
    result_dir.mkdir(parents=True, exist_ok=True)
    output_path = result_dir / "eval_annual.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    print("\n" + "=" * 70)
    print("ğŸ“Š å…¨å¹´åŠ æƒæŒ‡æ ‡ (æŒ‰å¤©æ•°åŠ æƒ)")
    print("=" * 70)
    for k, v in annual_weighted.items():
        print(f"  {k}: {v:.2f}")
    print(f"\nâœ“ å…¨å¹´è¯„ä¼°ç»“æœå·²ä¿å­˜: {output_path}")


if __name__ == "__main__":
    main()
